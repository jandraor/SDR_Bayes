---
title: "S1 Appendix"
output: 
  pdf_document:
    number_sections: true
header-includes:
  - \usepackage{booktabs}
urlcolor: blue
bibliography: bibliography.bib
---

```{r setup, include=FALSE, message=FALSE, echo=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

# Supporting tools

First, we load the libraries and custom functions that support the analysis.

```{r}
library(bayesplot)
library(cmdstanr)
library(dplyr)
library(extraDistr)
library(GGally)
library(ggplot2)
library(ggpubr)
library(ggridges)
library(gridExtra)
library(kableExtra)
library(lubridate)
library(Metrics)
library(patchwork)
library(posterior)
library(purrr)
library(readr)
library(readsdr)
library(scales)
library(stringr)
library(tidyr)
library(viridisLite)

# Custom functions
source("./R/helpers.R")
source("./R/plots.R")
source("./R/incidence_comparison.R")

# Backup folder
fldr <- "./backup_objs/main_text/"
dir.create(fldr, showWarnings = FALSE)
```

# Data

Drawing on the *Tidyverse* (readr, dplyr, lubridate), we read the csv file that
contains the incidence data and transform it into a format suitable for the
analysis. 

```{r, echo = TRUE, fig.height = 3.5}
flu_data <- read_csv("./data/Cumberland_data_1918.csv") %>% 
  rename(time = Time, y = Cases) %>% 
  mutate(Date = dmy(Date),
         Week = epiweek(Date))
```


## Daily data

In this graph and the remaining figures, *ggplot2* provides the framework for
creating visualisations. Here, we show the number of new flu cases in Cumberland
(Maryland) between September and October 1918.

```{r, fig.height = 3.5}
g <- ggplot(flu_data, aes(x = Date, y = y)) +
  geom_col(fill = "steelblue") +
  theme_pubclean() +
  labs(x = "Time (days)",
       y = "Incidence (People per day)")

print(g)
```

```{r, echo = FALSE}
ggsave("./plots/A_incidence_report.pdf", plot = g, dpi = "print", height = 5, 
       width = 8)
```

## Weekly data

Here, we convert the daily data into weekly data. 

```{r, fig.height = 3.5}
weekly_data <- flu_data %>% group_by(Week) %>% 
  summarise(y = sum(y)) %>% 
  rename(time = Week) %>% 
  mutate(time = time - min(time) + 1)

ggplot(weekly_data, aes(x = time, y = y)) +
  geom_col(fill = "slategray3") +
  theme_pubclean() +
  labs(title = "Weekly incidence", x = "Week", y = "Incidence [People / wk]")
```

# Calibration workflow

## Prior information - $\pi(\theta)$

The following list corresponds to the time-independent variables and initial
conditions of the SEIR model that will be fitted to the Cumberland data. To
each parameter, we indicate their prior knowledge. For parameters for which we
cannot obtain direct estimates, we construct prior distributions and present 
them in a density plot.

* Recovery rate: $\gamma$ = 1 / 2 [1 / day]

* Rate of onset of infectiousness: $\sigma$ = 1 / 2 [1 / day]

* Initial recovered: $R(0)$ = 0 [People]

* Initial exposed: $E(0)$ = 0 [People]

* Initial susceptible: $S(0)$) = 5234 - $I(0)$ [People]

* Initial infected: $I(0) \sim lognormal(0, 1)$ [People]

* Rate of effective contacts per infected individual: $\beta \sim lognormal(0, 1)$ [1 / day]

* Average reporting rate: $\rho \sim beta(2, 2)$ [1 / day]

```{r, fig.height = 2}
g1 <- ggplot(NULL, aes(c(0, 10))) + 
  geom_area(stat = "function", fun = dlnorm, fill = "grey95", 
            colour = "grey60") +
  scale_x_continuous(breaks = c(0, 5, 10)) +
  theme_pubr() +
  labs(y = "",
       x = bquote(beta)) +
  theme(axis.line.y = element_blank(),
        axis.ticks = element_blank(),
        axis.text.y = element_blank())

g2   <- ggplot(NULL, aes(c(0, 1))) + 
   geom_area(stat = "function", fun = dbeta, fill = "grey95", 
            colour = "grey60", args = list(shape1 = 2, shape2 = 2)) +  
  scale_x_continuous(breaks = c(0, 0.5, 1)) +
  theme_pubr() +
  labs(y = "",
       x = bquote(rho)) +
  theme(axis.line.y = element_blank(),
        axis.ticks = element_blank(),
        axis.text.y = element_blank())

g3   <- g1 + labs(y = "",
       x = "I(0)",
       title = "")

g <- g1 + g2 + g3

print(g)
```

```{r, echo = FALSE}
ggsave("./plots/A_prior_dist.pdf", dpi = "print", height = 3, width = 7)
```

## Prior predictive checks

Here, we incorporate the SEIR model built in Stella. We parse the XML code to
R using the function **read_xmile** from the *readsdr* package. Then, we
draw 500 samples from the prior distribution and simulate the model with these
inputs. Given the computational burden of this process, we save the results
in a RDS file.

```{r}
file_path <- file.path(fldr, "prior_sims.rds")

n_sims     <- 500
pop_size   <- 5234
const_list <- list(N = pop_size, sigma = 0.5, par_gamma = 0.5)
filepath   <- "./models/SEIR.stmx"
mdl        <- read_xmile(filepath, const_list = const_list)

if(!file.exists(file_path)) {
  set.seed(300194)
  
  I_0_sims   <- rlnorm(n_sims, 0, 1)
  rho_sims   <- rbeta(n_sims, 2, 2)
  beta_sims  <- rlnorm(n_sims, 0, 1)
  
  consts_df  <- data.frame(par_beta  = beta_sims,
                           rho       = rho_sims)
  R0         <- round(5234 * 0.3, 0)  # Initial value for the recovered stock
  stocks_df  <- data.frame(S = pop_size - R0 - I_0_sims,
                           I = I_0_sims,
                           C = I_0_sims)

  sens_o <- sd_sensitivity_run(mdl$deSolve_components, start_time = 0, 
                               stop_time = 91, timestep = 1 / 32,
                               multicore = TRUE, n_cores = 4, 
                               integ_method = "rk4", stocks_df = stocks_df,
                               consts_df = consts_df)
  saveRDS(sens_o, file_path)
} else {
  sens_o <- readRDS(file_path)
}

set.seed(346282)
sens_inc <- predictive_checks(n_sims, sens_o)
```

Here, we plot the results of the prior predictive checks. Dots indicate the
actual data.

```{r, fig.height = 3.5, fig.width = 7}
g1 <- ggplot(sens_inc, aes(x = time, y = y)) +
  geom_line(aes(group = iter), alpha = 0.1, colour = "grey50") +
  geom_point(data = flu_data, aes(x = time, y = y), size = 0.5, 
             colour = "steelblue") +
  theme_pubclean() +
  labs(y        = "Incidence [People / day]",
       x        = "Time [Days]",
       subtitle = "A)")

print(g1)
```

In the above simulations, it can be seen that the prior distribution produces
simulations far off the actual data. To have a closer inspection, we filter out
trajectories whose peak is greater than 200 new cases in a day.

```{r, fig.height = 3.5, fig.width = 7}
max_y_df    <- sens_inc %>% group_by(iter) %>% summarise(max_y = max(y))
filtered_df <- filter(max_y_df, max_y < 200)
iters       <- filtered_df$iter

sens_inc2 <- filter(sens_inc, iter %in% iters)

g2 <- ggplot(sens_inc2, aes(x = time, y = y)) +
  geom_line(aes(group = iter), alpha = 0.1, colour = "grey50") +
  geom_point(data = flu_data, aes(x = time, y = y), size = 0.5, 
             colour = "steelblue") +
  theme_pubclean() +
  labs(y        = "Incidence [People / day]",
       x        = "Time [Days]",
       subtitle = "B)")

print(g2)
```

```{r, echo = FALSE}
g <- g1 / g2

ggsave("./plots/A_prior_predictive_checks.pdf", plot = g, dpi = "print", 
       height = 5, width = 7)
```

### Measurement model

In this section, we discuss the choice of the measurement model, and how we 
use prior predictive checks to guide such a decision. Initially, we opt for the
default choice, i.e. the normal distribution. This choice implies that, at every
time step, the difference between the measurement from the true value (error) 
follows a normal distribution. In addition this, this distributions entails that
the error across all time steps is equal (homoscedasticity). In other words,  
the magnitude of the error is indifferent from the magnitude of the true 
value, an assumption that may seem unrealistic. As could be expected, the 
normal distribution adds a new unknown, the standard deviation ($\delta$). To 
test this model, we assume $\delta \sim Cauchy(0,1)$, and simulate 500
trajectories. We notice that this configuration yields continuous unconstrained
values, in contrast to the discrete non-negative measurements. These 
dissimilitudes hint that more suitable specifications should be found.

Next, we consider the Poisson distribution given that it has been used in the
empirical treatment of count data, particularly concerning counts of events per 
unit of time. This distribution lifts the constraint of equal variance across
measurements as the magnitude of the error is proportional to the measured 
quantity's magnitude. In this example, the true number of reported individuals.
As with the normal distribution, we generate 500 simulations. As we saw in the
previous section, the Poisson model captures adequately the given data.

Finally, to count data, the Negative Binomial distribution is an alternative to
the Poisson should the latter fails to capture overdispersion the data. We model
such overdispersion in the Negative Binomial's scale parameter
($\phi \sim \operatorname{Half-normal}(0,1)$). We can see that the trajectories
generated by the Negative Binomial are more dispersed than the ones produced by
the Poisson distribution. Nevertheless, in this case, they do not represent
more accurately the data, so that we adopt the Poisson distribution as the
measurement model.


```{r, fig.height = 6}
set.seed(102667)

pois_meas <- sens_inc %>% mutate(dist = "Poisson")
# Normal distributed measurements
norm_meas   <- predictive_checks(n_sims, sens_o, "norm") %>% 
  mutate(dist = "Normal")

# Negative binomial measurements
nbinom_meas <- predictive_checks(n_sims, sens_o, "nbinom") %>% 
  mutate(dist = "Neg binom")

meas_df <- bind_rows(pois_meas, norm_meas, nbinom_meas)

n1 <- ggplot(meas_df, aes(x = time, y = y)) +
  geom_line(aes(group = iter), alpha = 0.1, colour = "grey50") +
  geom_point(data = flu_data, aes(x = time, y = y), size = 0.5, 
             colour = "steelblue") +
  facet_wrap(~dist, ncol = 1, scales = "free") +
  theme_pubclean() +
  labs(y        = "Incidence [People / day]",
       x        = "Time [Days]",
       subtitle = "")

print(n1)
```



## Fitting

For calibrating SD models in Stan, it is necessary to create a file written in
Stan's own language. This kind of file structures the code in blocks. For this
example, five blocks are necessary: Functions, data, parameters, transformed
parameters and model. Here model refers to the measurement model (prior and
likelihood). The SD model is considered a function, which can be constructed 
from the Stella file. This is possible thanks to the function 
*stan_ode_function* from the *readsdr* package. Similarly, *readsdr* supports
the creation of the data block. The other blocks must be built manually. 

```{r}
pars_hat  <- c("I0", "rho", "beta")
gamma     <- 0.5
sigma     <- 0.5
consts    <- sd_constants(mdl)
ODE_fn    <- "SEIR"
stan_fun  <- stan_ode_function(filepath, ODE_fn, pars = consts$name[c(2, 1)], 
                               const_list = list(N = pop_size, par_gamma = gamma,
                                                 sigma = sigma))

fun_exe_line <- str_glue("  o = ode_rk45({ODE_fn}, x0, t0, ts, params);") 
```

```{r}
stan_data <- stan_data("y", type = "int", inits = FALSE)

stan_params <- paste(
  "parameters {",
  "  real<lower = 0>            beta;",
  "  real<lower = 0, upper = 1> rho;",
  "  real<lower = 0> I0;",
"}", sep = "\n")

stan_tp <- paste(
  "transformed parameters{",
  "  vector[n_difeq] o[n_obs]; // Output from the ODE solver",
  "  real x[n_obs];",
  "  vector[n_difeq] x0;",
  "  real params[n_params];",
  "  x0[1] = 3664 - I0;",
  "  x0[2] = 0;",
  "  x0[3] = I0;",
  "  x0[4] = 1570;",
  "  x0[5] = I0;",
  "  params[1] = beta;",
  "  params[2] = rho;",
  fun_exe_line,
  "  x[1] =  o[1, 5]  - x0[5];",
  "  for (i in 1:n_obs-1) {",
  "    x[i + 1] = o[i + 1, 5] - o[i, 5] + 1e-5;",
  "  }",
  "}", sep = "\n")

stan_model <- paste(
  "model {",
  "  rho   ~ beta(2, 2);",
  "  beta  ~ lognormal(0, 1);",
  "  I0    ~ lognormal(0, 1);",
  "  y     ~ poisson(x);", 
  "}",
  sep = "\n")

stan_text   <- paste(stan_fun, stan_data, stan_params,
                       stan_tp, stan_model, sep = "\n")

stan_fldr     <- "./Stan_files/example"
dir.create(stan_fldr, showWarnings = FALSE)  
stan_filepath <- file.path(stan_fldr, "flu_poisson.stan")

create_stan_file(stan_text, stan_filepath)
```

We show below the code contained in the Stan file.

```{r}
cat(stan_text)
```

To perform the calibration via HMC, we must provide Stan with the calibration
parameters. We specify 2,000 iterations (1,000 for warming-up and 1,000 for 
sampling), and 4 chains. We also supply the number of parameters to be fitted
for the SD model, the number of stocks (_n_difeq_), the simulation time and the
Cumberland data.

```{r fit_norm, results = 'hide'}
fldr      <- "./backup_objs/main_text/"
file_path <- file.path(fldr, "fit.rds")

if(!file.exists(file_path)) {
  stan_d <- list(n_obs    = nrow(flu_data),
               y        = flu_data$y,
               n_params = 2,
               n_difeq  = 5,
               t0       = 0,
               ts       = 1:length(flu_data$y))

mod <- cmdstan_model(stan_filepath)

fit <- mod$sample(data            = stan_d,
                  seed            = 553616,
                  chains          = 4,
                  parallel_chains = 4,
                  iter_warmup     = 1000,
                  iter_sampling   = 1000,
                  refresh         = 5,
                  save_warmup     = TRUE,
                  output_dir      = fldr)

fit$save_object(file_path)
} else {
  fit <- readRDS(file_path)
}
```

### Diagnostics

Before inspecting the samples, Stan provides global diagnostics to the user 
about the sampling process. It is expected that the result is free from
divergent iterations or indications of pathological behaviour from the
Bayesian Fraction of Missing Information metric. Also, iterations that
saturate the maximum tree depth indicate a complex likelihood surface. The 
[Stan manual](https://mc-stan.org/misc/warnings.html#runtime-warnings) provides
intuitive interpretations of these metrics.


```{r, message = TRUE}
# We generated this file from $fit$cmdstan_diagnose()
fileName <- file.path(fldr, "diags.txt")

readChar(fileName, file.info(fileName)$size) %>% 
  cat()
```


#### Trace plots

\hfill

A common approach to inspect calibration results is to check for convergence
in trace plots.

```{r, fig.height = 3}


a <- trace_plot(fit, n_samples = 100, "A)") +
  theme(axis.text = element_text(size = 5))

b <- trace_plot(fit, subtitle = "B)") +
  theme(axis.text = element_text(size = 5))

print(a + b)
```

```{r, echo = FALSE}
ggsave("./plots/A_traceplot.pdf", plot = a + b, dpi = "print", height = 3, 
       width = 8)
```

#### Potential reduction factor ($\widehat{R}$) & Effective Sample Size ($\hat{n}_{eff}$)  

\hfill

$\widehat{R}$ is a convergence diagnostic, which compares the between- and 
within-chain estimates for model parameters and other univariate quantities of
interest. If chains have not mixed well, R-hat is larger than 1. It is 
recommended running at least *four chains* by default and only using the sample
if R-hat is less than 1.01 [@Vehtari_2021]. **Stan reports R-hat which is the
maximum of rank normalized split-R-hat and rank normalized folded-split-R-hat,
which works for thick tailed distributions and is sensitive also to differences 
in scale**.

For each parameter $\theta$, we split each chain from the *sampling phase* in 
two halves. That is, from 4 chains of 1000 draws each one, we obtain 8 split
chains of 500 draws each one. Then, we label the simulations as 
$\theta_{ij} (i = 1, . . . , N; j = 1, . . . , M)$, where $N$ is the number of 
samples per split chain, $M$ is the number of split chains, and $S = NM$ is the
total number of draws from all chains. We subsequently transform these 
simulations to their corresponding rank normalized values $z_{ij}$. According to
@Vehtari_2021, we replace each value $\theta_{ij}$ by its rank $r_{ij}$ within 
the pooled draws from all chains. Second, we transform ranks to normal scores 
using the inverse normal transformation and a fractional offset via Equation 1:

\begin{equation}
z_{ij} = \Phi^{-1}\left(\frac{r_{ij} - 3/8}{S - 1/4}\right)
\end{equation}

Using these normal scores, we calculate $\widehat{R}$ following the formulation
proposed by [@gelman2013bayesian]. Initially, we compute $B$ and $W$ , the 
between- and within-sequence variances, respectively:

\begin{equation}
B = \frac{N}{M-1}\sum^{M}_{j = 1}(\bar{z}_{.j} - \bar{z}_{..})^2,
\;where\; \bar{z}_{.j} = \frac{1}{n}\sum^{n}_{i = 1}z_{ij},\;
\bar{z}_{..} = \frac{1}{M} \sum^{M}_{j=1} \bar{z}_{.j}
\end{equation}

\begin{equation}
W = \frac{1}{M}\sum_{j = 1}^{M}s_{j}^2,\; where\;
s_{j}^2 = \frac{1}{N-1}\sum^{n}_{i = 1}(z_{ij} - \bar{z}_{.j})^2
\end{equation}

Then, we can estimate $\widehat{var}^{+}(\theta|y)$, the marginal posterior
variance of the parameter, by a weighted average of $W$ and $B$:

\begin{equation}
\widehat{var}^{+}(\theta|y) = \frac{N-1}{N}W + \frac{1}{N}B
\end{equation}

From (3) and (4), we obtain the rank normalized split $\widehat{R}$:

\begin{equation}
\widehat{R} = \sqrt{\frac{\widehat{var}^{+}(\theta|y)}{W}}
\end{equation}


To obtain the rank normalized folded-split $\widehat{R}$, we simply transform
the simulations (Equation 6) and then apply the procedure described above 
(Equations 1-5).

\begin{equation}
\zeta_{ij} = |\theta_{ij} - median(\theta)|
\end{equation}

For MCMC draws, we define the estimated effective sample size as

\begin{equation}
\hat{n}_{eff} = \frac{MN}{1 + 2 \sum_{t=1}^{T}\hat{\rho}_t}
\end{equation}

This quantity requires an estimate of the sum of the correlations $\rho$
up to lag T (the first odd positive integer for which 
${\hat{\rho}}_{T+1} + {\hat{\rho}}_{T+2}$ is negative). The correlation at any 
specific lag $t$ (Equation 8) depends upon the estimate $\widehat{var}^{+}$ 
and the *Variogram* at each $t$ (Equation 9).

\begin{equation}
\hat{\rho_t} = 1 - \frac{V_t}{2\widehat{var}^{+}} 
\end{equation}

\begin{equation}
V_t = \frac{1}{M(N - t)} \sum_{j=1}^{M}\sum_{i=t+1}^{N}(z_{i,j} - z_{i -t, j})^2 
\end{equation}

We use the term *bulk effective sample size* to refer to the effective sample 
size based on the rank normalized draws. To ensure reliable estimates of 
variances and autocorrelations needed for $\hat{R}$ and $\hat{n}_{eff}$, @Vehtari_2021 
recommend that the rank-normalized effecive sample size must be greater than 400
(100 per chain).


```{r}
unk_pars <- c("beta", "rho", "I0")

par_matrices <- lapply(unk_pars, function(par) {
  extract_variable_matrix(fit$draws(), par)
})

ess_vals   <- map_dbl(par_matrices, ess_bulk)
r_hat_vals <- map_dbl(par_matrices, rhat)

smy_df  <- data.frame(par      = c("beta", "rho", "I(0)"),
                      ess_bulk = ess_vals,
                      rhat     = r_hat_vals)
```

```{r, echo = FALSE}
kable(smy_df, booktab = TRUE)
```


#### Check deterministic fit

\hfill

Once the computation has been deemed satisfactory, we subsequently check that
the deterministic model captures the underlying trajectory of the data. In this
case, the trajectory corresponds to the **unobserved** expected reported
incidence, and in turn to the expected value of the model. Accordingly, we 
evaluate the predicted *expected* reported incidences from the calibration
process. Take into account that in each sampling iteration, Stan simulates a 
expected incidence trajectory. We compare these simulated trajectories against 
the measured incidence(dots) to visually examine whether the trend is consistent 
with the data. The solid blue line denotes the mean trajectory and the grey
shaded area indicates the 95 % credible interval.

```{r, fig.height = 3.5}
posterior_df <- fit$draws() %>% as_draws_df()

samples_normal <- posterior_df[ , pars_hat] %>% 
  mutate(R0 = beta / gamma, ll = "Normal")

y_hat_df_norm <- extract_timeseries_var("x", posterior_df)

summary_df <- y_hat_df_norm  %>% group_by(time) %>% 
  summarise(lb = quantile(value, c(0.025, 0.975)[[1]]),
            ub = quantile(value, c(0.025, 0.975)[[2]]),
            y  = mean(value))

ggplot(summary_df, aes(x = time , y)) +
  geom_ribbon(aes(ymin = lb, ymax = ub), fill = "grey90") +
  geom_line(colour = "steelblue") +
  geom_point(data = flu_data) +
  theme_pubclean() +
  labs(y = "Incidence [New cases / day]", x = "Time [Days]")
```

We also evaluate the fit against the weekly data. It can be seen that the 
aggregation filters out the variation in daily data.

```{r, fig.height = 3.5}
weekly_sum <- y_hat_df_norm %>% mutate(week = time %/% 7 + 1) %>% 
  group_by(iter, week) %>% 
  summarise(.groups = "drop", value = sum(value)) %>% 
  group_by(week) %>% 
  summarise(lb = quantile(value, c(0.025, 0.975)[[1]]),
            ub = quantile(value, c(0.025, 0.975)[[2]]),
            y  = mean(value)) %>% 
  rename(time = week)

ggplot(weekly_sum, aes(x = time, y = y)) +
  geom_ribbon(aes(ymin = lb, ymax = ub), fill = "grey90") +
  geom_line(colour = "steelblue") +
  geom_point(data = weekly_data) +
  theme_pubclean() +
  labs(x = "Time [Week]",
       y = "Incidence [New cases / week]")
```

### Posterior information

#### Prior & posterior comparison

\hfill

One important goal of model calibration is to gain information from the data
about the parameters of interest. One way to accomplish such purpose is by
comparing marginal prior and marginal posterior distributions. Namely, we 
evaluate the knowledge acquired from the process.

```{r, fig.height = 3}
pars_df <- posterior_df[, c("beta", "rho", "I0")]
g       <- prior_posterior_plot(pars_df)
```

```{r, echo = FALSE}
ggsave("./plots/A_prior_pos_comparison.pdf",plot = g, dpi = "print", 
       height = 3, 
       width = 7)
```


#### Pair plots

\hfill

This plot evaluates parameter interactions or joint distributions. The lower 
triangular shows heat maps of the concentration of values in the x-y plane among
all possible parameter pairs. The upper triangular quantifies such interactions 
by the correlation coefficients. The diagonal displays marginal posterior 
distributions.

```{r}
g <- pairs_posterior(pars_df, strip_text = 10)
print(g)
```


```{r, echo = FALSE}
ggsave("./plots/A_pairs_plot.pdf", plot = g, dpi = "print", height = 4, 
       width = 8)
```

#### Posterior predictive checks

\hfill

The other main goal of model calibration is related to the argument that it is a
stringent validity test of SD models. Simply put, we ask "Is the structure
capable of generating an approximation of the historical behavior?". To answer
this question, we draw 500 samples from the posterior distribution that serve as
inputs to the SD model. Unlike the check for the deterministic fit, this step
includes the measurement error, which we assumed was Poisson distributed. Thus,
we insert the expected reported incidences into the Poisson distribution to
predict measured incidences. In A), we show the 500 predicted measurements and
the actual data (dots). The dotted line indicates the expected reported incidence.
In B), we show three examples of the predicted measurements. 

```{r}
file_path <- file.path(fldr, "posterior_sims.rds")

set.seed(1800)
row_ids      <- sample.int(nrow(pars_df), 500, replace = TRUE)
post_samples <- pars_df[row_ids, ]

if(!file.exists(file_path)) {
  
  consts_df <- dplyr::select(post_samples, beta, rho) %>% 
    rename(par_beta = beta)

  stocks_df <- dplyr::select(post_samples, I0) %>% 
    rename(I = I0) %>% 
    mutate(S = pop_size - I - R0,
           R = R0,
           C = I)

  sens_o <- sd_sensitivity_run(mdl$deSolve_components, start_time = 0, 
                               stop_time = 91, timestep = 1 / 32,
                               multicore = TRUE, n_cores = 4, 
                               integ_method = "rk4", stocks_df = stocks_df,
                               consts_df = consts_df)
  
  saveRDS(sens_o, file_path)
} else {
  sens_o <- readRDS(file_path)
}

# Posterior predictive checks
ppc <- predictive_checks(n_sims, sens_o)
```

```{r}
expected_value <- ppc %>% group_by(time) %>% 
  summarise(y = mean(y))

sample_traj <- sample.int(n_sims, 3) %>% sort()
ppc_rest    <- ppc %>% filter(!iter %in% sample_traj)

ppc1        <- ppc %>% filter(iter == sample_traj[[1]])
ppc2        <- ppc %>% filter(iter == sample_traj[[2]])
ppc3        <- ppc %>% filter(iter == sample_traj[[3]])

line_cols   <- viridisLite::viridis(3)

g <- ggplot(ppc_rest, aes(x = time, y = y)) +
  geom_line(aes(group = iter), alpha = 0.01, colour = "grey50") +
  geom_point(data = flu_data, aes(x = time, y = y), size = 1,
            colour = "steelblue", alpha = 0.75) +
  geom_line(data = expected_value, colour = "black", linetype = "dashed") +
  geom_line(data = ppc1, colour = line_cols[[1]], alpha = 0.9) +
  geom_line(data = ppc2, colour = line_cols[[2]], alpha = 0.9) +
  geom_line(data = ppc3, colour = line_cols[[3]], alpha = 0.9) +
  theme_pubclean() +
  labs(y     = "Incidence [People / day]",
       x     = "Days")

print(g)
```

```{r, echo = FALSE}
ggsave("./plots/A_ppc.pdf", plot = g, dpi = "print", height = 5, 
       width = 8)
```


#### Mean Absolute Scaled Error

\hfill

This definition is taken from [Hyndman(2006)](https://robjhyndman.com/papers/foresight.pdf):

*The MASE was proposed by Hyndman and Koehler (2006) as a generally applicable
measurement of forecast accuracy. They proposed  scaling  the  errors  based  on
the  in-sample Mean Absolute Error (MAE) from  the  na誰ve  forecast  method. 
Using  the  na誰ve  method, we generate one-period-ahead forecasts from each data
point in the sample. Accordingly, a scaled error is defined as:*

\begin{equation}
 q_t = \frac{e_t}{\frac{1}{n-1} \sum\limits_{i=2}^n |y_i - y_{i-1}|}
\end{equation}

where the numerator $e_t$ is the absolute forecast error ($|\hat{y}_t -y_t|$) 
for a specific time. Here $\hat{y_t}$ denotes the predicted data from
the fitted model and $y_t$ the actual data. Further, $n$ represents the number
of data points. The denominator is the MAE from the one-step "naive forecast
method", defined as the actual value ($y_t$) minus the forecast value 
($y_{t-1}$) for t > 1. 

Thus, the mean absolute scaled error is:

\begin{equation}
 MASE = mean(|q_t|)
\end{equation}

This is a scale-independent metric defined if there are zero values. A scaled
error  is  less  than  one  if  it  arises  from  a  better  forecast than
the  na誰ve  forecast. Conversely, it is greater than one if the forecasts
worse  than  the  average  one-step,  na誰ve  forecast computed  in-sample.

Based on the above, we estimate the MASE for each simulated trajectory and 
present its distribution.

```{r}
df_list <- split(ppc, f = ppc$iter)
mases   <- sapply(df_list, function(df) mase(flu_data$y, df$y))
MASE_df <- data.frame(MASE = mases)

g2 <- ggplot(MASE_df, aes(x = MASE)) +
  geom_density(fill = "grey95") +
  scale_x_continuous(limits = c(0.60, 1)) +
  geom_vline(xintercept = 1, linetype = "dotted") +
  theme_pubr() +
  labs(y = "Density") +
  theme(axis.line.y = element_blank(),
        axis.ticks = element_blank(),
        axis.text.y = element_blank())

print(g2)
```

# Forecast

Finally, we use the posterior distribution to simulate two hypothetical 
scenarios (Unmitigated and Intervention). Thus, we simulate 500 trajectories
for each scenario. We also evaluate the effect of assuming perfect independence
among parameters (500 simulations per scenario). In total, we simulate 2,000
trajectories.

```{r}
file_path <- file.path(fldr, "forecast.rds")

if(!file.exists(file_path)) {

  total_pop    <- 10000
  pars_df2     <- mutate(pars_df, beta = beta * 0.6)

  set.seed(12)

  unm_inc <- incidence_comparison(pars_df, total_pop) %>% 
    mutate(Scenario = "Unmitigated")

  itv_inc <- incidence_comparison(pars_df2, total_pop) %>% 
    mutate(Scenario = "Intervention")

  inc_df <- bind_rows(unm_inc, itv_inc) %>% 
    mutate(Scenario = factor(Scenario, levels = c("Unmitigated" ,"Intervention")))

  saveRDS(inc_df, file_path)
} else {
  inc_df <- readRDS(file_path)
}
```

```{r}
ggplot(inc_df, aes(x = time, y = y)) +
  geom_line(colour = "grey75", aes(group = iter), alpha = 0.3) +
  facet_grid(Scenario ~ Case) +
  theme_pubclean() +
  labs(title = "Incidence by scenario",
       y     = "Incidence [People / day]",
       x     = "Days")
```

To visually inspect the uncertainty added by the independence assumption, we 
estimate 95 % credible intervals at each time step, generating the contours
shown below. We superimpose the contours for each scenario. The external blue
border corresponds to the added uncertainty.

```{r}
summary_forecast <- inc_df %>%
  mutate(Case = factor(Case, levels = c("Independent parameters", 
                                        "Correlated parameters"))) %>% 
  group_by(time,Case, Scenario) %>% 
  summarise(lb = quantile(y, 0.025),
            ub = quantile(y, 0.975))

ggplot(summary_forecast, aes(x = time)) +
  geom_ribbon(aes(ymin = lb, ymax = ub, fill = Case), alpha = 0.5) +
  theme_pubclean() +
  scale_fill_manual(values = c("steelblue", "grey50")) +
  facet_wrap(~Scenario)
```

```{r, echo = FALSE}
ggsave("./plots/A_prediction.pdf", dpi = "print", height = 5, 
       width = 8)
```

# Original Computing Environment

```{r}
sessionInfo()
```

# References
