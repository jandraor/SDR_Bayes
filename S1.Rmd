---
title: "Supplementary Information"
output: 
  pdf_document:
    number_sections: true
header-includes:
  - \usepackage{booktabs}
urlcolor: blue
---

```{r setup, include=FALSE, message=FALSE, echo=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

# Supporting tools

First, we load the libraries and custom functions that support the analysis.

```{r}
library(bayesplot)
library(cmdstanr)
library(dplyr)
library(extraDistr)
library(GGally)
library(ggplot2)
library(ggpubr)
library(ggridges)
library(gridExtra)
library(kableExtra)
library(lubridate)
library(Metrics)
library(patchwork)
library(purrr)
library(readr)
library(readsdr)
library(rstan)
library(scales)
library(stringr)
library(tidyr)

# Custom functions
source("./R/helpers.R")
source("./R/plots.R")
source("./R/incidence_comparison.R")
```

# Data

Drawing on the *Tidyverse* (readr, dplyr, lubridate), we read the csv file that
contains the incidence data and transform it into a format suitable for the
analysis. 

```{r, echo = TRUE, fig.height = 3.5}
flu_data <- read_csv("./data/Cumberland_data_1918.csv") %>% 
  rename(time = Time, y = Cases) %>% 
  mutate(Date = dmy(Date),
         Week = epiweek(Date))
```


## Daily data

In this graph and the remaining figures, *ggplot2* provides the framework for
creating visualisations. Here, we show the number of new flu cases in Cumberland
(Maryland) between September and October 1918.

```{r, fig.height = 3.5}
g <- ggplot(flu_data, aes(x = Date, y = y)) +
  geom_col(fill = "steelblue") +
  theme_pubclean() +
  labs(x = "Time (days)",
       y = "Incidence (People per day)")

print(g)
```

```{r, echo = FALSE}
ggsave("./plots/A_incidence_report.pdf", plot = g, dpi = "print", height = 5, 
       width = 8)
```

## Weekly data

Here, we convert the daily data into weekly data. 

```{r, fig.height = 3.5}
weekly_data <- flu_data %>% group_by(Week) %>% 
  summarise(y = sum(y)) %>% 
  rename(time = Week) %>% 
  mutate(time = time - min(time) + 1)

ggplot(weekly_data, aes(x = time, y = y)) +
  geom_col(fill = "slategray3") +
  theme_pubclean() +
  labs(title = "Weekly incidence", x = "Week", y = "Incidence [People / wk]")
```

# Calibration workflow

## Prior information - $\pi(\theta)$

The following list corresponds to the time-independent variables and initial
conditions of the SEIR model that will be fitted to the Cumberland data. To
each parameter, we indicate their prior knowledge. For parameters for which we
cannot obtain direct estimates, we construct prior distributions and present 
them in a density plot.

* Recovery rate: $\gamma$ = 1 / 2 [1 / day]

* Rate of onset of infectiousness: $\sigma$ = 1 / 2 [1 / day]

* Initial recovered: $R(0)$ = 0 [People]

* Initial exposed: $E(0)$ = 0 [People]

* Initial susceptible: $S(0)$) = 5234 - $I(0)$ [People]

* Initial infected: $I(0) \sim lognormal(0, 1)$ [People]

* Rate of effective contacts per infected individual: $\beta \sim lognormal(0, 1)$ [1 / day]

* Average reporting rate: $\rho \sim beta(2, 2)$ [1 / day]

```{r, fig.height = 2}
g1 <- ggplot(NULL, aes(c(0, 10))) + 
  geom_area(stat = "function", fun = dlnorm, fill = "grey95", 
            colour = "grey60") +
  scale_x_continuous(breaks = c(0, 5, 10)) +
  theme_pubr() +
  labs(y = "",
       x = bquote(beta)) +
  theme(axis.line.y = element_blank(),
        axis.ticks = element_blank(),
        axis.text.y = element_blank())

g2   <- ggplot(NULL, aes(c(0, 1))) + 
   geom_area(stat = "function", fun = dbeta, fill = "grey95", 
            colour = "grey60", args = list(shape1 = 2, shape2 = 2)) +  
  scale_x_continuous(breaks = c(0, 0.5, 1)) +
  theme_pubr() +
  labs(y = "",
       x = bquote(rho)) +
  theme(axis.line.y = element_blank(),
        axis.ticks = element_blank(),
        axis.text.y = element_blank())

g3   <- g1 + labs(y = "",
       x = "I(0)",
       title = "")

g <- g1 + g2 + g3

print(g)
```

```{r, echo = FALSE}
ggsave("./plots/A_prior_dist.pdf", dpi = "print", height = 3, width = 7)
```

## Prior predictive checks

Here, we incorporate the SEIR model built in Stella. We parse the XML code to
R using the function **read_xmile** from the *readsdr* package. Then, we
draw 500 samples from the prior distribution and simulate the model with these
inputs. Given the computational burden of this process, we save the results
in a RDS file.

```{r}
file_path <- "./backup_objs/prior_sims.rds"

n_sims     <- 500
pop_size   <- 5234
const_list <- list(N = pop_size, sigma = 0.5, par_gamma = 0.5)
filepath   <- "./models/SEIR.stmx"
mdl        <- read_xmile(filepath, const_list = const_list)

if(!file.exists(file_path)) {
  set.seed(300194)
  
  I_0_sims   <- rlnorm(n_sims, 0, 1)
  rho_sims   <- rbeta(n_sims, 2, 2)
  beta_sims  <- rlnorm(n_sims, 0, 1)
  
  consts_df  <- data.frame(par_beta  = beta_sims,
                           rho       = rho_sims)
  R0         <- round(5234 * 0.3, 0)  # Initial value for the recovered stock
  stocks_df  <- data.frame(S = pop_size - R0 - I_0_sims,
                           I = I_0_sims,
                           C = I_0_sims)

  sens_o <- sd_sensitivity_run(mdl$deSolve_components, start_time = 0, 
                               stop_time = 91, timestep = 1 / 32,
                               multicore = TRUE, n_cores = 4, 
                               integ_method = "rk4", stocks_df = stocks_df,
                               consts_df = consts_df)
  saveRDS(sens_o, file_path)
} else {
  sens_o <- readRDS(file_path)
}

sens_inc <- predictive_checks(n_sims, sens_o)
```

Here, we plot the results of the prior predictive checks. Dots indicate the
actual data.

```{r, fig.height = 3.5, fig.width = 7}
g1 <- ggplot(sens_inc, aes(x = time, y = y)) +
  geom_line(aes(group = iter), alpha = 0.1, colour = "grey50") +
  geom_point(data = flu_data, aes(x = time, y = y), size = 0.5, 
             colour = "steelblue") +
  theme_pubclean() +
  labs(y        = "Incidence [People / day]",
       x        = "Time [Days]",
       subtitle = "A)")

print(g1)
```

In the above simulations, it can be seen that the prior distribution produces
simulations far off the actual data. To have a closer inspection, we filter out
trajectories whose peak is greater than 200 new cases in a day.

```{r, fig.height = 3.5, fig.width = 7}
max_y_df    <- sens_inc %>% group_by(iter) %>% summarise(max_y = max(y))
filtered_df <- filter(max_y_df, max_y < 200)
iters       <- filtered_df$iter

sens_inc2 <- filter(sens_inc, iter %in% iters)

g2 <- ggplot(sens_inc2, aes(x = time, y = y)) +
  geom_line(aes(group = iter), alpha = 0.1, colour = "grey50") +
  geom_point(data = flu_data, aes(x = time, y = y), size = 0.5, 
             colour = "steelblue") +
  theme_pubclean() +
  labs(y        = "Incidence [People / day]",
       x        = "Time [Days]",
       subtitle = "B)")

print(g2)
```

```{r, echo = FALSE}
g <- g1 / g2

ggsave("./plots/A_prior_predictive_checks.pdf", plot = g, dpi = "print", 
       height = 5, width = 7)
```

## Fitting

For calibrating SD models in Stan, it is necessary to create a file written in
Stan's own language. This kind of file structures the code in blocks. For this
example, five blocks are necessary: Functions, data, parameters, transformed
parameters and model. Here model refers to the measurement model (prior and
likelihood). The SD model is considered a function, which can be constructed 
from the Stella file. This is possible thanks to the function 
*stan_ode_function* from the *readsdr* package. Similarly, *readsdr* supports
the creation of the data block. The other blocks must be built manually. 

```{r}
pars_hat  <- c("I0", "rho", "beta")
gamma     <- 0.5
sigma     <- 0.5
consts    <- sd_constants(mdl)
ODE_fn    <- "SEIR"
stan_fun  <- stan_ode_function(filepath, ODE_fn, pars = consts$name[c(2, 1)], 
                               const_list = list(N = pop_size, par_gamma = gamma,
                                                 sigma = sigma))

fun_exe_line <- str_glue("  o = ode_rk45({ODE_fn}, x0, t0, ts, params);") 
```

```{r}
stan_data <- stan_data("y", type = "int", inits = FALSE)

stan_params <- paste(
  "parameters {",
  "  real<lower = 0>            beta;",
  "  real<lower = 0, upper = 1> rho;",
  "  real<lower = 0> I0;",
"}", sep = "\n")

stan_tp <- paste(
  "transformed parameters{",
  "  vector[n_difeq] o[n_obs]; // Output from the ODE solver",
  "  real x[n_obs];",
  "  vector[n_difeq] x0;",
  "  real params[n_params];",
  "  x0[1] = 3664 - I0;",
  "  x0[2] = 0;",
  "  x0[3] = I0;",
  "  x0[4] = 1570;",
  "  x0[5] = I0;",
  "  params[1] = beta;",
  "  params[2] = rho;",
  fun_exe_line,
  "  x[1] =  o[1, 5]  - x0[5];",
  "  for (i in 1:n_obs-1) {",
  "    x[i + 1] = o[i + 1, 5] - o[i, 5] + 1e-5;",
  "  }",
  "}", sep = "\n")

stan_model <- paste(
  "model {",
  "  rho   ~ beta(2, 2);",
  "  beta  ~ lognormal(0, 1);",
  "  I0    ~ lognormal(0, 1);",
  "  y     ~ poisson(x);", 
  "}",
  sep = "\n")

stan_text   <- paste(stan_fun, stan_data, stan_params,
                       stan_tp, stan_model, sep = "\n")

stan_filepath <- "./Stan_files/example/flu_poisson.stan"

create_stan_file(stan_text, stan_filepath)
```

We show below the code contained in the Stan file.

```{r}
cat(stan_text)
```

To perform the calibration via HMC, we must provide Stan with the calibration
parameters. We specify 2,000 iterations (1,000 for warming-up and 1,000 for 
sampling), and 4 chains. We also supply the number of parameters to be fitted
for the SD model, the number of stocks (_n_difeq_), the simulation time and the
Cumberland data.

```{r fit_norm, results = 'hide'}
file_path <- "./backup_objs/poisson_fit.rds"

if(!file.exists(file_path)) {
  stan_d <- list(n_obs    = nrow(flu_data),
               y        = flu_data$y,
               n_params = 2,
               n_difeq  = 5,
               t0       = 0,
               ts       = 1:length(flu_data$y))

mod <- cmdstan_model(stan_filepath)

fit <- mod$sample(data            = stan_d,
                  seed            = 553616,
                  chains          = 4,
                  parallel_chains = 4,
                  iter_warmup     = 1000,
                  iter_sampling   = 1000,
                  refresh         = 5,
                  save_warmup     = TRUE)

sf_poisson <- rstan::read_stan_csv(fit$output_files())
saveRDS(sf_poisson, file_path)
} else {
  sf_poisson <- readRDS(file_path)
}
```

### Diagnostics

Before inspecting the samples, Stan provides global diagnostics to the user 
about the sampling process. It is expected that the result is free from
divergent iterations or indications of pathological behaviour from the
Bayesian Fraction of Missing Information metric. Also, iterations that
saturate the maximum tree depth indicate a complex likelihood surface. The 
[Stan manual](https://mc-stan.org/misc/warnings.html#runtime-warnings) provides
intuitive interpretations of these metrics.

```{r, message = TRUE}
check_hmc_diagnostics(sf_poisson)
```


#### Trace plots

\hfill

A common approach to inspect calibration results is to check for convergence
in trace plots.

```{r, fig.height = 3}
a <- trace_plot(sf_poisson, n_samples = 100, "A)") +
  theme(axis.text = element_text(size = 5))

b <- trace_plot(sf_poisson, subtitle = "B)") +
  theme(axis.text = element_text(size = 5))

print(a + b)
```

```{r, echo = FALSE}
ggsave("./plots/A_traceplot.pdf", plot = a + b, dpi = "print", height = 3, 
       width = 8)
```

#### Effective Sample Size (ESS) & potential reduction factor ($\hat{R}$) 

\hfill

```{r}
smy_obj <- rstan::summary(sf_poisson, c("beta", "rho", "I0"))
smy_df  <- smy_obj$summary %>% as.data.frame()

kable(smy_df [, c("mean", "sd", "n_eff", "Rhat")], booktab = TRUE)
```

#### Check deterministic fit

\hfill

Once the computation has been deemed satisfactory, we subsequently check that
the deterministic model captures the underlying trajectory of the data. In this
case, the trajectory corresponds to the **unobserved** expected reported
incidence, and in turn to the expected value of the model. Accordingly, we 
evaluate the predicted *expected* reported incidences from the calibration
process. Take into account that in each sampling iteration, Stan simulates a 
expected incidence trajectory. We compare these simulated trajectories against 
the measured incidence(dots) to visually examine whether the trend is consistent 
with the data. The solid blue line denotes the mean trajectory and the grey
shaded area indicates the 95 % credible interval.

```{r, fig.height = 3.5}
posterior_df <- as.data.frame(sf_poisson)

samples_normal <- posterior_df[ , pars_hat] %>% 
  mutate(R0 = beta / gamma, ll = "Normal")

y_hat_df_norm <- extract_timeseries_var("x", posterior_df)

summary_df <- y_hat_df_norm  %>% group_by(time) %>% 
  summarise(lb = quantile(value, c(0.025, 0.975)[[1]]),
            ub = quantile(value, c(0.025, 0.975)[[2]]),
            y  = mean(value))

ggplot(summary_df, aes(x = time , y)) +
geom_ribbon(aes(ymin = lb, ymax = ub), fill = "grey90") +
  geom_line(colour = "steelblue") +
  geom_point(data = flu_data) +
  theme_pubclean() +
  labs(y = "Incidence [New cases / day]", x = "Time [Days]")
```

We also evaluate the fit against the weekly data. It can be seen that the 
aggregation filters out the variation in daily data.

```{r, fig.height = 3.5}
weekly_sum <- y_hat_df_norm %>% mutate(week = time %/% 7 + 1) %>% 
  group_by(iter, week) %>% 
  summarise(.groups = "drop", value = sum(value)) %>% 
  group_by(week) %>% 
  summarise(lb = quantile(value, c(0.025, 0.975)[[1]]),
            ub = quantile(value, c(0.025, 0.975)[[2]]),
            y  = mean(value)) %>% 
  rename(time = week)

ggplot(weekly_sum, aes(x = time, y = y)) +
  geom_ribbon(aes(ymin = lb, ymax = ub), fill = "grey90") +
  geom_line(colour = "steelblue") +
  geom_point(data = weekly_data) +
  theme_pubclean() +
  labs(x = "Time [Week]",
       y = "Incidence [New cases / week]")
```

### Posterior information

#### Prior & posterior comparison

\hfill

One important goal of model calibration is to gain information from the data
about the parameters of interest. One way to accomplish such purpose is by
comparing marginal prior and marginal posterior distributions. Namely, we 
evaluate the knowledge acquired from the process.

```{r, fig.height = 3}
pars_df <- posterior_df[, c("beta", "rho", "I0")]
g       <- prior_posterior_plot(pars_df)
```

```{r, echo = FALSE}
ggsave("./plots/A_prior_pos_comparison.pdf",plot = g, dpi = "print", 
       height = 3, 
       width = 7)
```


#### Pair plots

\hfill

This plot evaluates parameter interactions or joint distributions. The lower 
triangular shows heat maps of the concentration of values in the x-y plane among
all possible parameter pairs. The upper triangular quantifies such interactions 
by the correlation coefficients. The diagonal displays marginal posterior 
distributions.

```{r}
g <- pairs_posterior(pars_df, strip_text = 10)
print(g)
```


```{r, echo = FALSE}
ggsave("./plots/A_pairs_plot.pdf", plot = g, dpi = "print", height = 4, 
       width = 8)
```

#### Posterior predictive checks

\hfill

The other main goal of model calibration is related to the argument that it is a
stringent validity test of SD models. Simply put, we ask "Is the structure
capable of generating an approximation of the historical behavior?". To answer
this question, we draw 500 samples from the posterior distribution that serve as
inputs to the SD model. Unlike the check for the deterministic fit, this step
includes the measurement error, which we assumed was Poisson distributed. Thus,
we insert the expected reported incidences into the Poisson distribution to
predict measured incidences. In A), we show the 500 predicted measurements and
the actual data (dots). The dotted line indicates the expected reported incidence.
In B), we show three examples of the predicted measurements. 

```{r}
file_path <- "./backup_objs/posterior_sims.rds"

set.seed(1800)
row_ids      <- sample.int(nrow(pars_df), 500, replace = TRUE)
post_samples <- pars_df[row_ids, ]

if(!file.exists(file_path)) {
  
  consts_df <- dplyr::select(post_samples, beta, rho) %>% 
    rename(par_beta = beta)

  stocks_df <- dplyr::select(post_samples, I0) %>% 
    rename(I = I0) %>% 
    mutate(S = pop_size - I - R0,
           R = R0,
           C = I)

  sens_o <- sd_sensitivity_run(mdl$deSolve_components, start_time = 0, 
                               stop_time = 91, timestep = 1 / 32,
                               multicore = TRUE, n_cores = 4, 
                               integ_method = "rk4", stocks_df = stocks_df,
                               consts_df = consts_df)
  
  saveRDS(sens_o, file_path)
} else {
  sens_o <- readRDS(file_path)
}

# Posterior predictive checks
ppc <- predictive_checks(n_sims, sens_o)
```

```{r}
expected_value <- ppc %>% group_by(time) %>% 
  summarise(y = mean(y))

g1 <- ggplot(ppc, aes(x = time, y = y)) +
  geom_line(aes(group = iter), alpha = 0.1, colour = "grey50") +
  geom_point(data = flu_data, aes(x = time, y = y), size = 0.5,
            colour = "steelblue") +
  geom_line(data = expected_value, colour = "black", linetype = "dashed") +
  theme_pubclean() +
  labs(subtitle = "A)",
       y     = "Incidence [People / day]",
       x     = "Days")

sample_traj <- sample.int(n_sims, 3) %>% sort()



gs <- map2(sample_traj, c("B)", "", "") ,function(i, subtitle) {
  sample_df <- filter(ppc, iter == i)
  
  ggplot(sample_df, aes(x = time, y = y)) +
  geom_line(alpha = 0.8, colour = "grey20") +
  geom_point(data = flu_data, aes(x = time, y = y), size = 0.5, 
             colour = "steelblue", alpha = 0.5) +
  theme_pubclean() +
  labs(subtitle = subtitle,
       y     = "Incidence [People / day]",
       x     = "Days") +
  theme(legend.position = "none",
        axis.title = element_text(size = 6, colour = "grey45"),
        axis.text  = element_text(size = 4, colour = "grey45"))
})


patch_g <- g1 + (gs[[1]] / gs[[2]] / gs[[3]])
print(patch_g)
```

```{r, echo = FALSE}
ggsave("./plots/A_ppc.pdf", plot = patch_g, dpi = "print", height = 5, 
       width = 8)
```


#### Mean Absolute Scaled Error

\hfill

This definition is taken from [Hyndman(2006)](https://robjhyndman.com/papers/foresight.pdf):

*The MASE was proposed by Hyndman and Koehler (2006) as a generally applicable
measurement of forecast accuracy. They proposed  scaling  the  errors  based  on
the  in-sample Mean Absolute Error (MAE) from  the  naïve  forecast  method. 
Using  the  naïve  method, we generate one-period-ahead forecasts from each data
point in the sample. Accordingly, a scaled error is defined as:*

\begin{equation}
 q_t = \frac{e_t}{\frac{1}{n-1} \sum\limits_{i=2}^n |y_i - y_{i-1}|}
\end{equation}

where the numerator $e_t$ is the absolute forecast error ($|\hat{y}_t -y_t|$) 
for a specific time. Here $\hat{y_t}$ denotes the predicted data from
the fitted model and $y_t$ the actual data. Further, $n$ represents the number
of data points. The denominator is the MAE from the one-step "naive forecast
method", defined as the actual value ($y_t$) minus the forecast value 
($y_{t-1}$) for t > 1. 

Thus, the mean absolute scaled error is:

\begin{equation}
 MASE = mean(|q_t|)
\end{equation}

This is a scale-independent metric defined if there are zero values. A scaled
error  is  less  than  one  if  it  arises  from  a  better  forecast than
the  naïve  forecast. Conversely, it is greater than one if the forecasts
worse  than  the  average  one-step,  naïve  forecast computed  in-sample.

Based on the above, we estimate the MASE for each simulated trajectory and 
present its distribution.

```{r}
df_list <- split(ppc, f = ppc$iter)
mases   <- sapply(df_list, function(df) mase(flu_data$y, df$y))
MASE_df <- data.frame(MASE = mases)

g2 <- ggplot(MASE_df, aes(x = MASE)) +
  geom_density(fill = "grey95") +
  scale_x_continuous(limits = c(0.60, 1)) +
  geom_vline(xintercept = 1, linetype = "dotted") +
  theme_pubr() +
  labs(y = "Density") +
  theme(axis.line.y = element_blank(),
        axis.ticks = element_blank(),
        axis.text.y = element_blank())

print(g2)
```

# Forecast

Finally, we use the posterior distribution to simulate two hypothetical 
scenarios (Unmitigated and Intervention). Thus, we simulate 500 trajectories
for each scenario. We also evaluate the effect of assuming perfect independence
among parameters (500 simulations per scenario). In total, we simulate 2,000
trajectories.

```{r}
file_path <- "./backup_objs/forecast.rds"

if(!file.exists(file_path)) {

  total_pop    <- 10000
  pars_df2     <- mutate(pars_df, beta = beta * 0.6)

  set.seed(12)

  unm_inc <- incidence_comparison(pars_df, total_pop) %>% 
    mutate(Scenario = "Unmitigated")

  itv_inc <- incidence_comparison(pars_df2, total_pop) %>% 
    mutate(Scenario = "Intervention")

  inc_df <- bind_rows(unm_inc, itv_inc) %>% 
    mutate(Scenario = factor(Scenario, levels = c("Unmitigated" ,"Intervention")))

  saveRDS(inc_df, file_path)
} else {
  inc_df <- readRDS(file_path)
}
```

```{r}
ggplot(inc_df, aes(x = time, y = y)) +
  geom_line(colour = "grey75", aes(group = iter), alpha = 0.3) +
  facet_grid(Scenario ~ Case) +
  theme_pubclean() +
  labs(title = "Incidence by scenario",
       y     = "Incidence [People / day]",
       x     = "Days")
```

To visually inspect the uncertainty added by the independence assumption, we 
estimate 95 % credible intervals at each time step, generating the contours
shown below. We superimpose the contours for each scenario. The external blue
border corresponds to the added uncertainty.

```{r}
summary_forecast <- inc_df %>%
  mutate(Case = factor(Case, levels = c("Independent parameters", 
                                        "Correlated parameters"))) %>% 
  group_by(time,Case, Scenario) %>% 
  summarise(lb = quantile(y, 0.025),
            ub = quantile(y, 0.975))

ggplot(summary_forecast, aes(x = time)) +
  geom_ribbon(aes(ymin = lb, ymax = ub, fill = Case), alpha = 0.5) +
  theme_pubclean() +
  scale_fill_manual(values = c("steelblue", "grey50")) +
  facet_wrap(~Scenario)
```

```{r, echo = FALSE}
ggsave("./plots/A_prediction.pdf", dpi = "print", height = 5, 
       width = 8)
```

# Original Computing Environment

```{r}
sessionInfo()
```
